{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e3d1c05-8e87-4adc-be4a-f02f6637701b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Import Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "eb77d20c-2d12-4b85-af02-bc4c67bd4049",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Packages and load Data"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from itertools import combinations\n",
    "import math\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# read Loyalty# as string because it's an identifier and we never perform numerical operations on it\n",
    "df_customer = pd.read_csv(\"DM_AIAI_CustomerDB.csv\", dtype={\"Loyalty#\": str})\n",
    "df_flights = pd.read_csv(\"DM_AIAI_FlightsDB.csv\", dtype={\"Loyalty#\": str})\n",
    "\n",
    "# display(df_customer.info())\n",
    "display(df_flights.info())\n",
    "df_flights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "833c80ee-e701-451f-a0ec-6eb72fa0cd08",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "renaming columns"
    }
   },
   "outputs": [],
   "source": [
    "# Rename columns due to programming standards while keeping the meaning\n",
    "# df_customer.info()\n",
    "df_customer = df_customer.rename(columns={\n",
    "    'Loyalty#': 'loyalty_id',\n",
    "    'First Name': 'first_name',\n",
    "    'Last Name': 'last_name',\n",
    "    'Customer Name': 'customer_name',\n",
    "    'Country': 'country',\n",
    "    'Province or State': 'prov_or_state',\n",
    "    'City': 'city',\n",
    "    'Latitude': 'latitude',\n",
    "    'Longitude': 'longitude',\n",
    "    'Postal code': 'postal_code',\n",
    "    'Location Code': 'location_code',\n",
    "    'Gender': 'gender',\n",
    "    'Education': 'education',\n",
    "    'Income': 'income',\n",
    "    'Marital Status': 'marital_status',\n",
    "    'LoyaltyStatus': 'loyalty_status',\n",
    "    'EnrollmentDateOpening': 'enrollment_date',\n",
    "    'CancellationDate': 'cancellation_date',\n",
    "    'Customer Lifetime Value': 'lifetime_value',\n",
    "    'EnrollmentType': 'enrollment_type'\n",
    "})\n",
    "df_customer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "95b0bc2c-4543-4952-a369-749697c7546b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "renaming columns"
    }
   },
   "outputs": [],
   "source": [
    "df_flights = df_flights.rename(columns={\n",
    "    'Loyalty#': 'loyalty_id',\n",
    "    'Year': 'year',\n",
    "    'Month': 'month',\n",
    "    'YearMonthDate': 'year_month_date',\n",
    "    'NumFlights': 'num_flights',\n",
    "    'NumFlightsWithCompanions': 'num_flights_with_companions',\n",
    "    'DistanceKM': 'distance_km',\n",
    "    'PointsAccumulated': 'points_accumulated',\n",
    "    'PointsRedeemed': 'points_redeemed',\n",
    "    'DollarCostPointsRedeemed': 'dollar_cost_points_redeemed'\n",
    "})\n",
    "df_flights.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "413307b4-7b23-4081-8760-402968234ab2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# First basic Data Exploration of the two raw dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7c226a0a-0330-4f36-806c-d446ecb7f1dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO Prio low: maybe outsource in function and call it with both dataframes to avoid redundant code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d4ccc93-875d-476e-8bd4-04a30d73c7e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Exploration and basic cleaning of df_customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79391889-72ca-4348-bb69-36e759467c41",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "check null values, dtypes, convert"
    }
   },
   "outputs": [],
   "source": [
    "# replace potential empty strings with nan first (Missing values will be handled later when preparing the data for the model)\n",
    "df_customer = df_customer.replace(r'^\\s*$', np.nan, regex=True) # use regex to catch empty or whitespace-only strings\n",
    "df_customer.info()\n",
    "# => 20 null values in lifetime_value and Income, 14.327 null values in cancellation_date (still active loyalty members)\n",
    "\n",
    "# columns enrollment_date and cancellation_date need to be converted to Datetime (coerce sets invalid parsing to NaT)\n",
    "df_customer[\"enrollment_date\"] = pd.to_datetime(df_customer[\"enrollment_date\"], errors=\"coerce\")\n",
    "df_customer[\"cancellation_date\"] = pd.to_datetime(df_customer[\"cancellation_date\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "509442ee-2cf4-4c04-8039-efd0c04512e3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "check basic descriptive statistics"
    }
   },
   "outputs": [],
   "source": [
    "df_customer.describe(include=\"all\").T\n",
    "\n",
    "# Findings:\n",
    "#   loyalty_id: 100.011 to 999.999: weird because only 16.921 IDs\n",
    "#   first_name/last_name/customer_name: CN completely unique, 1.517 people with shared last names (families?)\n",
    "#   location data: Almost 1/3 from Ontario (Toronto), all from Canada\n",
    "#   latitude/longitude: std of Longitude is 22, Customers are widely spread east to west > large coverage of Canada\n",
    "#   gender/education/marital_status: Male/Female equally distributed, most are Bachelors and Married \n",
    "#   income: High difference of median/mean, high std, 25% = 0 > do they really have no income or is it missing/censored data? max income 99.981 > no outliers to right side, capped?\n",
    "#   enrollment_date/cancellation_date: Enrollment Range from 2015–2021 (7 years), 86.4% of all Customers still active \n",
    "#   lifetime_value: Mean 7.990, Std 5.780 > Some Customers are worth much more\n",
    "#   enrollment_type: Standard Dominates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "13725edd-93e8-4b9e-975b-e929563de2a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "check for duplicates and remove constant variables"
    }
   },
   "outputs": [],
   "source": [
    "### Remove duplicates and constant variables\n",
    "print(len(df_customer))\n",
    "print(f\"\\nDuplicate customer rows: {df_customer.duplicated().sum()}\\n\")\n",
    "print(f\"Duplicate customer on loyalty_id: {df_customer['loyalty_id'].duplicated().sum()}\\n\")\n",
    "\n",
    "# Before further data exploration, removal of duplicate customer loyalty_id (327 of 16.921) needed because no mapping possible\n",
    "duplicate_loyalty_ids = df_customer['loyalty_id'][df_customer['loyalty_id'].duplicated()] # Store them in variable to also remove them from df_flights\n",
    "df_customer = df_customer[~df_customer['loyalty_id'].isin(duplicate_loyalty_ids)]\n",
    "print(len(df_customer))\n",
    "\n",
    "# Remove column country here because canada is the only value and therefore the column is constant and useless for modeling\n",
    "df_customer = df_customer.drop(columns=['country'])\n",
    "df_customer = df_customer.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d0bfcb2-b5c1-4feb-a860-f7d936fab946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Exploration and basic cleaning of df_flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fc98b735-f953-4902-8a20-733c9e817b5f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "check null values, dtypes, convert"
    }
   },
   "outputs": [],
   "source": [
    "# replace potential empty strings with nan first (Missing values will be handled later when preparing the data for the model)\n",
    "df_flights = df_flights.replace(r'^\\s*$', np.nan, regex=True) # use regex to catch empty or whitespace-only strings\n",
    "df_flights.info()\n",
    "# => no null values\n",
    "\n",
    "# column year_month_date needs to be converted to Datetime (keep Year and Month as integers because its more memory efficient)\n",
    "df_flights[\"year_month_date\"] = pd.to_datetime(df_flights[\"year_month_date\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d24e8c25-0435-45b9-a269-966b71e3832b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "check basic descriptive statistics"
    }
   },
   "outputs": [],
   "source": [
    "df_flights.describe(include=\"all\").T\n",
    "\n",
    "# Findings:\n",
    "#   year_month_date: Range from 01-01-2019 to 01-12-2021 \n",
    "#   distance_km: max 42.000km in a month seems to be an outlier\n",
    "#   points_accumulated: Most customers dont accumulate lots of points, top flyers dominate, mean way higher than avg\n",
    "#   Redeemed: points_redeemed = 0, lots of customers never redeem points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a83daad6-8f57-45db-98c8-51dc3e094359",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "check for duplicates and remove"
    }
   },
   "outputs": [],
   "source": [
    "### Remove duplicates\n",
    "print(len(df_flights))\n",
    "# Drop removed loyalty_ids from df_customer from df_flights as well\n",
    "df_flights = df_flights[~df_flights['loyalty_id'].isin(duplicate_loyalty_ids)]\n",
    "print(len(df_flights))\n",
    "# => from 608436 to 596664\n",
    "\n",
    "print(f\"\\nDuplicate flight rows: {df_flights.duplicated().sum()}\\n\")\n",
    "# => No duplicate flight rows anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "728f00a4-318e-4561-89c8-92155beb99ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Check for overlaps or inconsistencies between the two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8bf233b5-ec7b-4ac6-9783-04ad203d61e4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "check for overlaps and remove"
    }
   },
   "outputs": [],
   "source": [
    "# Find matches between both dataframes based on loyalty_id\n",
    "cust_ids = set(df_customer[\"loyalty_id\"])\n",
    "flight_ids = set(df_flights[\"loyalty_id\"])\n",
    "\n",
    "# Matches\n",
    "common_ids = cust_ids & flight_ids\n",
    "only_in_customers = cust_ids - flight_ids\n",
    "only_in_flights = flight_ids - cust_ids\n",
    "\n",
    "print(f\"Total in customers: {len(cust_ids)}\")\n",
    "print(f\"Total in flights:   {len(flight_ids)}\")\n",
    "print(f\"Matching IDs:       {len(common_ids)}\")\n",
    "print(f\"Only in customers:  {len(only_in_customers)}\")\n",
    "print(f\"Only in flights:    {len(only_in_flights)}\\n\")\n",
    "# => 20 customers without flights (16594 in customer data but only 16574 in flight data)\n",
    "\n",
    "# Customers present in customer data but missing in flights\n",
    "missing_flight_customers = df_customer[df_customer['loyalty_id'].isin(only_in_customers)]\n",
    "print(f\"Customers in customers but not in flights: {len(missing_flight_customers)}\")\n",
    "display(missing_flight_customers)\n",
    "# => These 20 customers enrolled and cancelled on the exact same day\n",
    "\n",
    "### Remove sparse months of flights per customer before their (first flight | enrollment_date) to delete useless data # TODO check whether this changes something in EDA and is necessary for now or later when preparing the data for modelling\n",
    "# => Suggestion for Business: Adjust data management by not adding past empty data for new customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b989603d-86bb-43e0-8afc-6cc8d0dfe383",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Join data (aggregate flight data for each customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "41ac9cd0-06b3-4e41-9a4f-abc75acd5288",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "create df_customerflights"
    }
   },
   "outputs": [],
   "source": [
    "# Merge both dataframes to get a df containing all flights mapped to customer info (~600.000 entries: each customer has 36 rows, one per month)\n",
    "df_customerflights = df_flights.merge(df_customer, on=\"loyalty_id\", how=\"inner\") # inner join excludes the 20 customers without flights because they are not important analysis regarding flights\n",
    "print(len(df_customerflights))\n",
    "df_customerflights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a6264ee-e8d2-4eb0-ba2b-44514de72b41",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Agg numerical flight data per customer"
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate (sum) the numerical flight data per customer (~16.000 entries) \n",
    "# year, month and year_month_date are dropped: Not relevant for aggregation but for time series analysis later\n",
    "df_aggregated_flights_per_customer = df_flights.groupby(\"loyalty_id\").agg({\n",
    "    'num_flights': 'sum',\n",
    "    'num_flights_with_companions': 'sum',\n",
    "    'distance_km': 'sum',\n",
    "    'points_accumulated': 'sum',\n",
    "    'points_redeemed': 'sum',\n",
    "    'dollar_cost_points_redeemed': 'sum',\n",
    "}).reset_index().rename(columns={\n",
    "    'num_flights': 'total_num_flights',\n",
    "    'num_flights_with_companions': 'total_num_flights_with_companions',\n",
    "    'distance_km': 'total_distance_km',\n",
    "    'points_accumulated': 'total_points_accumulated',\n",
    "    'points_redeemed': 'total_points_redeemed',\n",
    "    'dollar_cost_points_redeemed': 'total_dollar_cost_points_redeemed'\n",
    "}) # reset_index to have loyalty_id as a column and rename the columns for clarity\n",
    "print(f'Customer dataframe ({len(df_customer)} entries):')\n",
    "display(df_customer.head())\n",
    "print(f'Aggregated (summed) flights grouped per customer dataframe ({len(df_aggregated_flights_per_customer)} entries):')\n",
    "display(df_aggregated_flights_per_customer.head())\n",
    "df_cf_aggregated = df_customer.merge(df_aggregated_flights_per_customer, on=\"loyalty_id\", how=\"left\")\n",
    "print(f'Merged customer and aggregated flights dataframe ({len(df_cf_aggregated)} entries):')\n",
    "display(df_cf_aggregated.head())\n",
    "display(df_cf_aggregated.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "83f8a9cd-7cfc-4984-a43c-72a21c06346f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Quick summary statistics"
    }
   },
   "outputs": [],
   "source": [
    "# Quick summary statistics of the merged dataframe before feature engineering and EDA of the final dataframe\n",
    "df_cf_aggregated.describe(include=\"all\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "873b006c-e620-4f85-8f46-81cdffff558b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data exploration of the joined dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b194cde7-ec0b-4aed-9d22-abd6375311ee",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "df_customerflights some data analysis + fuck ups"
    }
   },
   "outputs": [],
   "source": [
    "# how many customers have a cancellation_date?\n",
    "ex_loyalty_customers = df_cf_aggregated.loc[df_cf_aggregated[\"cancellation_date\"].notna(), \"loyalty_id\"]\n",
    "print(\"Number of Ex-Loyalty Customers:\", len(ex_loyalty_customers), \"\\n\")\n",
    "# => 2265 (while 20 of those have 0 flights and were enrolled for only one day -> maybe a mistake or some incentives for enrollment but then cancelled immediately)\n",
    "\n",
    "\n",
    "# how many customers have no flights at all?\n",
    "customers_no_flights = df_cf_aggregated[df_cf_aggregated['total_num_flights'] == 0]\n",
    "print(f\"Number of customers with zero flights: {len(customers_no_flights)}\\n\")\n",
    "# => 1473 customers with zero flights (probably they just enrolled but never used the loyalty program or cancelled immediately or are new customers)\n",
    "\n",
    "\n",
    "# how many customers have no cancellation_date but also no flights? -> enrolled but inactive (Cold Start Program)\n",
    "inactive_but_enrolled_customers_ = df_cf_aggregated[\n",
    "    (df_cf_aggregated[\"cancellation_date\"].notna()) &\n",
    "    (df_cf_aggregated[\"total_num_flights\"] == 0)\n",
    "]\n",
    "print(\"Number of inactive but enrolled Customers for Cold Start Program:\", len(inactive_but_enrolled_customers_), \"\\n\")\n",
    "# => 945 inactive but enrolled Customers for Cold Start Program\n",
    "\n",
    "\n",
    "# how many customers have a cancellation_date but have flights afterwards? > un-enrolled but still active (Welcome Back Program) (use df_customerflights to check monthly data)\n",
    "active_ex_customers = df_customerflights.groupby(\"loyalty_id\").filter(\n",
    "    lambda customer: customer[\"cancellation_date\"].notna().any() and \n",
    "              ((customer[\"num_flights\"] > 0) & (customer[\"year_month_date\"] > customer[\"cancellation_date\"])).any()\n",
    ")\n",
    "print(\"Number of active ex-Customers for Welcome Back Program:\", active_ex_customers[\"loyalty_id\"].nunique(), \"\\n\")\n",
    "# => 174 un-enrolled but still active (Welcome Back Program)\n",
    "\n",
    "\n",
    "# Customers with Flights before their enrollment_date (use df_customerflights to check monthly data)\n",
    "customers_with_flights_before_enrollment = df_customerflights[\n",
    "    (df_customerflights[\"year_month_date\"] < df_customerflights[\"enrollment_date\"]) &\n",
    "    (df_customerflights[\"num_flights\"] > 0)\n",
    "][\"loyalty_id\"].unique()\n",
    "print(\"Number of Customers with Flights before Enrollment:\", len(customers_with_flights_before_enrollment), \"\\n\")\n",
    "# => 4981 Customers with flights before enrollment\n",
    "\n",
    "\n",
    "# Customers with cancellation_date before their enrollment_date\n",
    "customers_with_cancellation_before_enrollment = df_cf_aggregated[df_cf_aggregated[\"cancellation_date\"] < df_cf_aggregated[\"enrollment_date\"]]\n",
    "print(\"Number of Customers with Cancellation before Enrollment:\", len(customers_with_cancellation_before_enrollment), \"\\n\")\n",
    "# => 199 customers with cancellation before enrollment (does not make sense and should be checked with business -> probably wrong data entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9776a1f-d2a0-4dc2-9946-fabf4b3e5c2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e90b7442-d9e4-465e-a431-a405f76ddee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## WIP ~Jan: On df_cf_aggregated (to be removed or finished entirely (Prio low))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4b481685-82e5-4af7-b62c-b73446810af5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "add aggregate columns (total, avg, median, std)"
    }
   },
   "outputs": [],
   "source": [
    "# Adding necessary aggregations of columns from the flights dataset as new features (total, avg, median, std)\n",
    "# Build a lot of features first and remove them later through feature selection as discussed in the lecture\n",
    "\n",
    "# Flights\n",
    "median_flights_per_month = df_flights.groupby('loyalty_id')['num_flights'].median()\n",
    "std_flights_per_month = df_flights.groupby('loyalty_id')['num_flights'].std()\n",
    "flights_aggs = pd.DataFrame({\n",
    "    'median_flights_per_month': median_flights_per_month,\n",
    "    'std_flights_per_month': std_flights_per_month\n",
    "})\n",
    "df_cf_aggregated = df_cf_aggregated.merge(flights_aggs, on='loyalty_id', how='inner')\n",
    "\n",
    "# # Uncomment this display-check to check whether both calculations for df_cf_aggregated and df_customerflights are the same\n",
    "# display(df_cf_aggregated[['loyalty_id', 'avg_flights_per_month', 'median_flights_per_month', 'std_flights_per_month']].sort_values('loyalty_id').head())\n",
    "# display(df_customerflights[['loyalty_id', 'avgflightsmonth', 'median_flights_per_month', 'std_flights_per_month']].drop_duplicates(subset=['loyalty_id']).sort_values('loyalty_id').head())\n",
    "\n",
    "# TODO Prio low: to be continued by Jan from here to have all engineered features in the aggregated df immediately instead of the df_customerflights\n",
    "\n",
    "display(df_cf_aggregated.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e60253f-13cb-4cf7-8239-b3b0e1badf49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## On df_customerflights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfb72848-18ab-4943-8f81-15bb5e0312ba",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "add necessary aggregations"
    }
   },
   "outputs": [],
   "source": [
    "# Adding necessary aggregations of columns from the flights dataset as new features (total, median, std)\n",
    "# Build a lot of features first and remove them later through feature selection as discussed in the lecture\n",
    "\n",
    "# Flights\n",
    "# Mean is not calculated because its the total value divided by all months (36) so their are perfectly correlated leading to zero new information\n",
    "df_customerflights['total_flights'] = df_customerflights.groupby('loyalty_id')['num_flights'].transform('sum')\n",
    "df_customerflights['median_flights_per_month'] = df_customerflights.groupby('loyalty_id')['num_flights'].transform('median')\n",
    "df_customerflights['std_flights_per_month'] = df_customerflights.groupby('loyalty_id')['num_flights'].transform('std')\n",
    "\n",
    "# # Uncomment to check that mean and total/36 is the same information\n",
    "# df_customerflights['avgflightsmonth'] = df_customerflights.groupby('loyalty_id')['num_flights'].transform('mean').round(2)\n",
    "# df_customerflights['total_flights_div_36'] = (df_customerflights['total_flights'] / 36).round(2)\n",
    "# df_customerflights[df_customerflights['avgflightsmonth'] != df_customerflights['total_flights_div_36']]\n",
    "# display(df_customerflights[['loyalty_id', \"avgflightsmonth\", 'total_flights_div_36']][df_customerflights['avgflightsmonth'] == df_customerflights['total_flights_div_36']])\n",
    "\n",
    "# Flights with Companions (Number of flights with companions within a month)\n",
    "df_customerflights['total_flights_w_comp'] = df_customerflights.groupby('loyalty_id')['num_flights_with_companions'].transform('sum')\n",
    "df_customerflights['median_flights_w_comp'] = df_customerflights.groupby('loyalty_id')['num_flights_with_companions'].transform('median')\n",
    "df_customerflights['std_flights_w_comp'] = df_customerflights.groupby('loyalty_id')['num_flights_with_companions'].transform('std')\n",
    "\n",
    "# Distance\n",
    "df_customerflights['total_distance_km'] = df_customerflights.groupby('loyalty_id')['distance_km'].transform('sum')\n",
    "df_customerflights['median_distance_km'] = df_customerflights.groupby('loyalty_id')['distance_km'].transform('median')\n",
    "df_customerflights['std_distance_km'] = df_customerflights.groupby('loyalty_id')['distance_km'].transform('std')\n",
    "\n",
    "# Points Accumulated\n",
    "df_customerflights['total_points_acc'] = df_customerflights.groupby('loyalty_id')['points_accumulated'].transform('sum')\n",
    "df_customerflights['median_points_acc'] = df_customerflights.groupby('loyalty_id')['points_accumulated'].transform('median')\n",
    "df_customerflights['std_points_acc'] = df_customerflights.groupby('loyalty_id')['points_accumulated'].transform('std')\n",
    "\n",
    "# Points Redeemed\n",
    "df_customerflights['total_points_redeemed'] = df_customerflights.groupby('loyalty_id')['points_redeemed'].transform('sum')\n",
    "df_customerflights['median_points_redeemed'] = df_customerflights.groupby('loyalty_id')['points_redeemed'].transform('median')\n",
    "df_customerflights['std_points_redeemed'] = df_customerflights.groupby('loyalty_id')['points_redeemed'].transform('std')\n",
    "\n",
    "# Dollar Cost Points Redeemed\n",
    "df_customerflights['total_dollar_redeemed'] = df_customerflights.groupby('loyalty_id')['dollar_cost_points_redeemed'].transform('sum')\n",
    "df_customerflights['median_dollar_redeemed'] = df_customerflights.groupby('loyalty_id')['dollar_cost_points_redeemed'].transform('median')\n",
    "df_customerflights['std_dollar_redeemed'] = df_customerflights.groupby('loyalty_id')['dollar_cost_points_redeemed'].transform('std')\n",
    "\n",
    "display(df_customerflights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9b99360-1c4f-46b6-9e80-23a37d8e5bf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO maybe squared features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3669089a-bc1e-43e5-bb01-3526f157deaa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "add further columns"
    }
   },
   "outputs": [],
   "source": [
    "### Engineered features using the total_flights:\n",
    "# add a column ratio_comp_per_flight that shows the ratio of flights with companions to total flights\n",
    "df_customerflights['ratio_comp_per_flight'] = np.where(\n",
    "    df_customerflights['total_flights'] > 0,\n",
    "    (df_customerflights['total_flights_w_comp'] / df_customerflights['total_flights']).round(4),\n",
    "    np.nan\n",
    ")\n",
    "df_customerflights['ratio_comp_per_flight'].describe()\n",
    "# => This feature could provide insights about customers that often fly with companions (e.g. families, couples)\n",
    "\n",
    "# add a column that provides insights on the average distance flown per flight\n",
    "df_customerflights['avg_distance_per_flight'] = np.where(\n",
    "    df_customerflights['total_flights'] > 0,\n",
    "    (df_customerflights['total_distance_km'] / df_customerflights['total_flights']).round(2),\n",
    "    np.nan\n",
    ")\n",
    "# => This feature could provide insights about customers that often fly long vs. short distances\n",
    "\n",
    "# add a column that provides insights on the average total_points_acc selected per flight\n",
    "df_customerflights['avg_points_acc_per_flight'] = np.where(\n",
    "    df_customerflights['total_flights'] > 0,\n",
    "    (df_customerflights['total_points_acc'] / df_customerflights['total_flights']).round(2),\n",
    "    np.nan\n",
    ")\n",
    "# => This feature could provide insights about customers that often accumulate many points per flight # TODO check correlation with avg_distance_per_flight\n",
    "\n",
    "# add a column that shows the clv per flight (only makes sense if total_flights is not part of the formula for clv)\n",
    "df_customerflights['lifetime_value_per_flight'] = np.where(\n",
    "    df_customerflights['total_flights'] > 0,\n",
    "    (df_customerflights['lifetime_value'] / df_customerflights['total_flights']).round(2),\n",
    "    np.nan\n",
    ")\n",
    "df_customerflights['lifetime_value_per_flight'].describe()\n",
    "df_customerflights['lifetime_value'].describe()\n",
    "# => This feature could provide insights about how valuable a customer is per flight taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "acfabe2c-f4f1-41ef-b3a3-1f2d0f1c8e83",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "add columns"
    }
   },
   "outputs": [],
   "source": [
    "# add column is_enrolled (bool, enrollment_date but no cancellation_date)\n",
    "df_customerflights['is_enrolled'] = df_customerflights['cancellation_date'].isna() & df_customerflights['enrollment_date'].notna()\n",
    "\n",
    "# add column recency (int, in months, time since last num_flights)\n",
    "df_customerflights = df_customerflights.sort_values(['loyalty_id', 'year_month_date'], ascending=[True, False])\n",
    "target_date = pd.Timestamp('2021-12-01')\n",
    "df_customerflights['recency'] = pd.NA\n",
    "\n",
    "# add column recency, group by loyaltyid, yearmonthdate desc, count rows until num_flights != 0\n",
    "for loyalty_id, group in df_customerflights.groupby('loyalty_id'):\n",
    "    if target_date in group['year_month_date'].values:\n",
    "        rec = 0\n",
    "        for f in group['num_flights'].values:\n",
    "            if f > 0:\n",
    "                break\n",
    "            rec += 1\n",
    "        idx = group[group['year_month_date'] == target_date].index\n",
    "        df_customerflights.loc[idx, 'recency'] = rec\n",
    "\n",
    "df_customerflights['recency'] = df_customerflights['recency'].astype('Int64') # Store as int instead of object\n",
    "\n",
    "# add column active_months (int, in months, count number of rows where num_flights >0)\n",
    "df_customerflights['active_months'] = df_customerflights.groupby('loyalty_id')['num_flights']\\\n",
    "    .transform(lambda x: (x > 0).sum())\n",
    "\n",
    "# add column enrolled_since_days (int, in days, time since enrollment_date until 2021-12-30 (max value of enrollment_date while max of df_flights is 2021-12-01))\n",
    "df_customerflights['enrolled_since_days'] = ((pd.Timestamp('2021-12-30') - df_customerflights['enrollment_date']) / np.timedelta64(1, 'D')).astype('Int64')\n",
    "print(df_customerflights['enrollment_date'].max())\n",
    "# => This feature could provide insights about clv and total_flights\n",
    "\n",
    "# add column cancelled_since_days (int, in days, time since cancellation_date until 2021-12-30 (max value of cancellation_date while max of df_flights is 2021-12-01))\n",
    "df_customerflights['cancelled_since_days'] = np.where(\n",
    "    df_customerflights['cancellation_date'].notna(),\n",
    "    ((pd.Timestamp('2021-12-30') - df_customerflights['cancellation_date']) / np.timedelta64(1, 'D')).astype('Int64'),\n",
    "    pd.NA\n",
    ")\n",
    "print(df_customerflights['cancellation_date'].max())\n",
    "# => This feature could provide insights about retention time and loyalty programs\n",
    "\n",
    "# add a column that shows the ratio of points redeemed to points accumulated\n",
    "df_customerflights['ratio_points_redeemed'] = np.where(\n",
    "    df_customerflights['total_points_acc'] > 0,\n",
    "    (df_customerflights['total_points_redeemed'] / df_customerflights['total_points_acc']).round(4),\n",
    "    np.nan\n",
    ")\n",
    "df_customerflights['ratio_points_redeemed'].describe() # TODO the max value of 33 is not possible as it is > 1\n",
    "# => This feature could provide insights about customer engagement in the loyalty program (=> activate people that do not redeem points to get the to collect more too and therefore fly more often with AIAI)\n",
    "\n",
    "\n",
    "# TODO add a feature clv per distance => could provide insights into why the clv does not make sense (business: several short flights could sum up to one long-distance flights in km but not in value for the company) => challenge the company formula for clv and find out state-of-the-art clv formula -> clv should be aligned with money and not with km\n",
    "\n",
    "# add a column that shows the frequency as a ratio of a customer since enrollment (\"active months since enrollment\") # TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "492b57ab-41d3-4d4d-b2bd-0306539d370a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Exploration of final dataframe including the engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "85a1651d-e4c4-477b-a802-4e856aada8df",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "creation of df_cf_unique"
    }
   },
   "outputs": [],
   "source": [
    "### Creation of df_cf_unique (final dataframe)\n",
    "# Due to the reason that df_customer only contains current data, a mapping of the customer to all their historic flight data is not useful. a combination of customer data together with total and average flight data is needed.\n",
    "df_cf_unique = df_customerflights.drop_duplicates(subset=['loyalty_id'], keep='first').drop(\n",
    "    columns=['year', 'month', 'year_month_date', 'num_flights', 'num_flights_with_companions',\n",
    "             'distance_km', 'points_accumulated', 'points_redeemed', 'dollar_cost_points_redeemed']\n",
    ")\n",
    "# => The 20 customers who didn't fly at all and were only present in the customers.csv but not in the flights.csv are not included in the final dataframe for analysis\n",
    "\n",
    "display(df_cf_unique.head())\n",
    "df_cf_unique.info()\n",
    "# => Final dataframe for further analysis with 16.574 unique customers with aggregated flight data\n",
    "\n",
    "df_cf_unique.describe(include=\"all\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6b16c121-4439-404a-8d51-c71b2d980eb3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "df_cf_unique some data analysis + fuck ups"
    }
   },
   "outputs": [],
   "source": [
    "# TODO add this EDA under the creation of the df_cf_unique to do this before the feature engineering\n",
    "# Customers in df_cf_unique with enrollment_date before 2021 but enrollment_type \"2021 Promotion\"\n",
    "customers_enroll_before_2021_but_promotion = df_cf_unique[\n",
    "    (df_cf_unique[\"enrollment_date\"] < \"2021-01-01\") &\n",
    "    (df_cf_unique[\"enrollment_type\"] == \"2021 Promotion\")\n",
    "][\"loyalty_id\"]\n",
    "print(\"Number of Customers enrolled before 2021 but with '2021 Promotion':\", len(customers_enroll_before_2021_but_promotion), \"\\n\")\n",
    "\n",
    "# Customers where points_redeemed > points_accumulated\n",
    "customers_points_redeemed_greater_than_totalpoints = df_cf_unique[df_cf_unique[\"total_points_redeemed\"] > df_cf_unique[\"total_points_acc\"]\n",
    "][\"loyalty_id\"]\n",
    "print(\"Number of Customers with pointsredeemed > pointsaccumulated:\", len(customers_points_redeemed_greater_than_totalpoints), \"\\n\") # TODO verbindung zu dem max=33 wert bei der ratio herstellen\n",
    "\n",
    "# Customers where total_flights_w_comp > num_flights\n",
    "customers_totalflightscomp_greater_than_totalflights = df_cf_unique[\n",
    "    df_cf_unique[\"total_flights_w_comp\"] > df_cf_unique[\"total_flights\"]\n",
    "][\"loyalty_id\"]\n",
    "print(\"Number of Customers with total_flights_w_comp > total_flights:\", len(customers_totalflightscomp_greater_than_totalflights), \"\\n\")\n",
    "\n",
    "# Extreme Distances in flights (customers with very little and very long distances per flight) # TODO maybe this is redundant with later outlier detection\n",
    "extreme_avg_flight_distance = df_cf_unique[(df_cf_unique['avg_distance_per_flight'] < 50) | \n",
    "                                (df_cf_unique['avg_distance_per_flight'] > 30000)]\n",
    "display(extreme_avg_flight_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "468cf7ec-5ffb-4f08-9fd5-c8ea57a3b07a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Exploration"
    }
   },
   "outputs": [],
   "source": [
    "# --- Exploration (Flights dispersion, Date correlations, Recency) ---\n",
    "\n",
    "# 1) Basic uniqueness\n",
    "print(f\"Unique total_flights values: {df_customerflights['total_flights'].nunique():,}\")\n",
    "\n",
    "# 2) Std of flights distribution (fix possible typo 'std_flights_per_month' -> 'stdflights')\n",
    "std_col = 'stdflights' if 'stdflights' in df_customerflights.columns else 'std_flights_per_month'\n",
    "print(df_customerflights[std_col].describe())\n",
    "\n",
    "# Are there any rows with std==0 but total_flights>0? (should be zero if your comment is true)\n",
    "mask_inconsistent = (df_customerflights[std_col] == 0) & (df_customerflights['total_flights'] > 0)\n",
    "print(f\"Rows with {std_col}==0 & total_flights>0: {mask_inconsistent.sum()}\")\n",
    "\n",
    "# Distribution\n",
    "plt.figure()\n",
    "plt.hist(df_customerflights[std_col].fillna(0), bins=40)\n",
    "plt.title(f'Distribution of {std_col}')\n",
    "plt.xlabel(std_col)\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "print()\n",
    "\n",
    "# 3) Correlations between enrollment_date and outcomes (convert dates -> numeric)\n",
    "cf = df_cf_unique.copy()\n",
    "cf['enrollment_date'] = pd.to_datetime(cf['enrollment_date'], errors='coerce')\n",
    "\n",
    "# Numeric proxy for date: days since 1970-01-01 (keeps scale reasonable)\n",
    "cf['enroll_days'] = (cf['enrollment_date'] - pd.Timestamp('1970-01-01')).dt.days\n",
    "\n",
    "# Ensure numeric targets\n",
    "for col in ['total_flights', 'lifetime_value']:\n",
    "    cf[col] = pd.to_numeric(cf[col], errors='coerce')\n",
    "\n",
    "pearson_corr_enroll_totalflights = cf['enroll_days'].corr(cf['total_flights'], method='pearson')\n",
    "spearman_corr_enroll_totalflights = cf['enroll_days'].corr(cf['total_flights'], method='spearman')\n",
    "print(f\"Pearson corr(enrollment_date, total_flights): {pearson_corr_enroll_totalflights:.4f}\")\n",
    "print(f\"Spearman corr(enrollment_date, total_flights): {spearman_corr_enroll_totalflights:.4f}\")\n",
    "\n",
    "pearson_corr_enroll_clv = cf['enroll_days'].corr(cf['lifetime_value'], method='pearson')\n",
    "spearman_corr_enroll_clv = cf['enroll_days'].corr(cf['lifetime_value'], method='spearman')\n",
    "print(f\"Pearson corr(enrollment_date, lifetime_value): {pearson_corr_enroll_clv:.4f}\")\n",
    "print(f\"Spearman corr(enrollment_date, lifetime_value): {spearman_corr_enroll_clv:.4f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# 4) Recency profile (spike at 0 often = seasonality / data freshness)\n",
    "rec_desc = df_cf_unique['recency'].describe()\n",
    "n_zero = (df_cf_unique['recency'] == 0).sum()\n",
    "print(rec_desc)\n",
    "print(f\"Customers with recency == 0: {n_zero:,} \"\n",
    "      f\"({n_zero / len(df_cf_unique):.1%} of customers)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee48b29-92e5-4f37-ba0f-5f610e363f56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Plotting the distributions (hists and boxplots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5ccda6fc-38d0-48d6-aeef-c8ec7351dbe9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "univariate distribution of numerical columns with histograms"
    }
   },
   "outputs": [],
   "source": [
    "# numeric features, excluding unwanted columns: loyalti_id is not necessary to plot and longitude, latitude have the same information as city and are not useful to plot as numerical values => They are useful for later geographical analysis \n",
    "exclude_cols = [\"latitude\", \"longitude\", \"loyalty_id\"]\n",
    "metric_features = [col for col in df_cf_unique.select_dtypes(include=[np.number]).columns \n",
    "                   if col not in exclude_cols]\n",
    "\n",
    "# TODO remove only histogram olots because we see them below next to the boxplots\n",
    "\n",
    "def plot_histograms(df, features, bins, n_cols=3):\n",
    "    \"\"\"Plot clean histograms for numeric features with given number of bins.\"\"\"\n",
    "    sns.set_theme(style=\"white\")  # white background\n",
    "    \n",
    "    n_rows = int(np.ceil(len(features) / n_cols))\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 3*n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for ax, feat in zip(axes, features):\n",
    "        vals = df[feat].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        if len(vals) > 0:\n",
    "            ax.hist(vals, bins=bins, color=\"steelblue\", edgecolor=\"black\")\n",
    "            ax.set_title(feat, fontsize=11, pad=8)\n",
    "            ax.set_xlabel(\"\")\n",
    "            ax.set_ylabel(\"Frequency\")\n",
    "        else:\n",
    "            ax.set_visible(False)\n",
    "    \n",
    "    # hide unused subplots\n",
    "    for ax in axes[len(features):]:\n",
    "        ax.set_visible(False)\n",
    "    \n",
    "    plt.suptitle(f\"Histograms of Numeric Variables ({bins} bins)\", fontsize=14, y=1.02)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# two clean versions for scientific report # TODO check bins rule of thumb from data mining lab\n",
    "plot_histograms(df_cf_unique, metric_features, bins=30)\n",
    "plot_histograms(df_cf_unique, metric_features, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9f44770f-0212-4300-ab5a-ba3bb32097e8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "histograms together with boxplots"
    }
   },
   "outputs": [],
   "source": [
    "# numeric features, excluding unwanted columns\n",
    "exclude_cols = [\"latitude\", \"longitude\", \"loyalty_id\"]\n",
    "metric_features = [col for col in df_cf_unique.select_dtypes(include=[np.number]).columns \n",
    "                   if col not in exclude_cols]\n",
    "\n",
    "def plot_hist_and_box(df, features, bins=20):\n",
    "    \"\"\"Plot histogram + boxplot side by side for each numeric feature.\"\"\"\n",
    "    sns.set_theme(style=\"white\")  # clean style\n",
    "    \n",
    "    for feat in features:\n",
    "        vals = df[feat].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        if len(vals) == 0:\n",
    "            continue\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 4), gridspec_kw={'width_ratios':[3, 1]})\n",
    "        \n",
    "        # Histogram\n",
    "        axes[0].hist(vals, bins=bins, color=\"steelblue\", edgecolor=\"black\")\n",
    "        axes[0].set_title(f\"Histogram of {feat}\", fontsize=12, pad=8)\n",
    "        axes[0].set_ylabel(\"Frequency\")\n",
    "        axes[0].set_xlabel(feat)\n",
    "        \n",
    "        # Boxplot\n",
    "        sns.boxplot(y=vals, ax=axes[1], color=\"steelblue\")\n",
    "        axes[1].set_title(f\"Boxplot of {feat}\", fontsize=12, pad=8)\n",
    "        axes[1].set_ylabel(\"\")\n",
    "        \n",
    "        # Layout\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Run for all numeric features\n",
    "plot_hist_and_box(df_cf_unique, metric_features, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df71bc31-3af0-4775-8b87-2a2ad0f055d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Bivariate Analysis\n",
    "\n",
    "This section summarizes key distribution patterns, associations, and segment differences. We first assess rank-based associations (Spearman), then apply simple log-transformations to heavily skewed variables (columns prefixed with `log`). Afterward we revisit linear associations (Pearson). We also include categorical–categorical associations (Cramér’s V), categorical–numeric association ratio (η), pairwise numeric relationships, and compact segment comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f0143b3e-79c8-4744-bcfa-4b89479d5c49",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Df for bivariate analysis"
    }
   },
   "outputs": [],
   "source": [
    "# Define df for bivariate analysis\n",
    "df_bivariate_analysis = df_cf_unique\n",
    "\n",
    "numeric_cols = [\n",
    "    \"total_flights\", \"median_flights_per_month\", \"std_flights_per_month\",\n",
    "    \"total_flights_w_comp\", \"ratio_comp_per_flight\", \"median_flights_w_comp\", \"std_flights_w_comp\",\n",
    "    \"total_distance_km\", \"avg_distance_per_flight\", \"median_distance_km\", \"std_distance_km\",\n",
    "    \"total_points_acc\", \"avg_points_acc_per_flight\", \"median_points_acc\", \"std_points_acc\", \"total_points_redeemed\", \"total_dollar_redeemed\",\n",
    "    \"lifetime_value\", \"income\", \"active_months\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "cd640217-c24c-40db-be73-98ec1cddf30d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Basic plot style"
    }
   },
   "outputs": [],
   "source": [
    "# Basic plot style for consistency\n",
    "sns.set_theme(style='whitegrid', font_scale=1.05)\n",
    "palette = sns.color_palette('deep')\n",
    "\n",
    "def has_cols(df, cols):\n",
    "    \"\"\"Quick check: are all given columns in df?\"\"\"\n",
    "    return all(c in df.columns for c in cols)\n",
    "\n",
    "# Identify numeric columns.\n",
    "# If a list was specified earlier, use it. Otherwise detect them.\n",
    "try:\n",
    "    numeric_features = numeric_cols\n",
    "except NameError:\n",
    "    numeric_features = [\n",
    "        c for c in df_bivariate_analysis.columns\n",
    "        if pd.api.types.is_numeric_dtype(df_bivariate_analysis[c])\n",
    "    ]\n",
    "\n",
    "# Categorical features I'm actually working with later.\n",
    "categorical_features = [\n",
    "    c for c in [\n",
    "        'gender', 'marital_status', 'education',\n",
    "        'loyalty_status', 'enrollment_type', 'is_enrolled'\n",
    "    ]\n",
    "    if c in df_bivariate_analysis.columns\n",
    "]\n",
    "\n",
    "print(f\"Numeric features found: {len(numeric_features)}\")\n",
    "print(f\"Categorical features found: {len(categorical_features)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b82806a3-de0b-4713-8658-4ffea128bd4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Pairplot (All Numeric)\n",
    "Pairwise relationships across numeric variables. To avoid heavy rendering on very large datasets, a small random sample is used if necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2b54fc1a-a6b0-4f02-b2ac-fc3443ee34c8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "pairplot  numeric columns"
    }
   },
   "outputs": [],
   "source": [
    "# Pairplot only for numeric columns that are actually present\n",
    "nums_exist = [c for c in numeric_features if c in df_bivariate_analysis.columns]\n",
    "pp_df = df_bivariate_analysis[nums_exist].dropna()\n",
    "\n",
    "# Avoid plotting huge data (keeps the chart readable + fast)\n",
    "if len(pp_df) > 3000:\n",
    "    pp_df = pp_df.sample(3000, random_state=42)\n",
    "\n",
    "g = sns.pairplot(pp_df, diag_kind='kde', corner=True)\n",
    "g.fig.suptitle('Numeric Features — Pairwise Relationships', y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c5d48b3-76f1-401c-b236-65942fe274cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa55401e-5a2a-4f17-be81-9e2987992469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Spearman Correlation\n",
    "Rank-based correlation across numeric variables (robust to monotonic but non-linear relationships).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80168064-e46c-45fa-a83e-caa5b2e8fe2b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "spearman correlation heatmap"
    }
   },
   "outputs": [],
   "source": [
    "# Spearman correlation for numeric features only\n",
    "num_for_corr = [c for c in numeric_features if c in df_bivariate_analysis.columns]\n",
    "\n",
    "if len(num_for_corr) > 1:\n",
    "    corr_spear = df_bivariate_analysis[num_for_corr].dropna().corr(method='spearman')\n",
    "\n",
    "    plt.figure(figsize=(min(0.55 * len(num_for_corr) + 5, 14),\n",
    "                        min(0.55 * len(num_for_corr) + 5, 14)))\n",
    "\n",
    "    sns.heatmap(\n",
    "        corr_spear,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cmap=\"vlag\",\n",
    "        center=0,\n",
    "        linewidths=.5,\n",
    "        annot_kws={\"size\": 10},\n",
    "        alpha=0.65,\n",
    "    )\n",
    "\n",
    "    plt.title('Spearman Correlation — Numeric Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Not enough numeric features for a correlation matrix.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd08f6f3-dca6-4efa-9dc9-1f722bc2b988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Interpretation of Spearman Correlation — Key Insights\n",
    "\n",
    "### Strong Correlations (as expected)\n",
    "- **Flight activity metrics strongly interrelated**\n",
    "  - `total_flights` ↔ `median_flights_per_month` / `std_fllights_per_month` (ρ ≈ 0.72–0.93)\n",
    "  - → Frequent flyers travel regularly and consistently\n",
    "- **More flights → more distance → more points**\n",
    "  - `total_distance_km` ↔ `total_flights` (ρ ≈ 0.80)\n",
    "  - `total_points_acc` ↔ `total_distance_km` (ρ ≈ 1.00)\n",
    "  - → Loyalty points earned scale with travel volume\n",
    "- **Points redemption relates to engagement**\n",
    "  - `total_points_redeemed` shows strong correlations with flights, distance, and points accumulated\n",
    "  - → Customers who travel more also redeem significantly more points\n",
    "- **Customer tenure drives value**\n",
    "  - `active_months` ↔ `total_flights` (ρ ≈ 0.89)\n",
    "  - → Longer relationships → more activity\n",
    "- **Competing flights correlate with own flights**\n",
    "  - `total_flights_w_comp` ↔ `total_flights` (ρ ≈ 0.73)\n",
    "  - → High-value travelers diversify across airlines → retention challenge\n",
    "\n",
    "\n",
    "### Unexpectedly Weak Correlations\n",
    "- **Income shows almost no relationship with travel**\n",
    "  - `income` ↔ flights/points (ρ ≈ 0.00–0.05)\n",
    "  - → Might be influenced by business travelers or noisy income ranges\n",
    "- **Lifetime Value not strongly tied to activity**\n",
    "  - `lifetime_value` ↔ `total_flights` (ρ ≈ 0.01)\n",
    "  - → Suggests other LTV components beyond pure flight revenue (or maybe not?)\n",
    "\n",
    "\n",
    "> Overall, travel volume, distance, and tenure dominate loyalty value —  \n",
    "> while income and redemption do **not** predict behavior as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "794304ab-90e5-40bb-a635-0a963a65277a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55c9b72b-bb11-44a1-8aa3-08dc3c373a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Transformations (Log) and Distribution Comparison\n",
    "For variables that were previously transformed (columns named like `log_<var>`), we show a before/after comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "97d8b87f-da81-4b1a-bd26-8da268f2571d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "transformation of  skewed data"
    }
   },
   "outputs": [],
   "source": [
    "# Some of the Data is skewed to the right according to the histograms above and therefore hurting the normal distribution assumption of the pearson correlation\n",
    "## we use log transformation to approximate a normal distribution\n",
    "\n",
    "df_bivariate_analysis[\"log(total_points_redeemed)\"] = np.log(df_bivariate_analysis[\"total_points_redeemed\"])\n",
    "df_bivariate_analysis[\"log(total_dollar_redeemed)\"] = np.log(df_bivariate_analysis[\"total_dollar_redeemed\"])\n",
    "df_bivariate_analysis[\"log(lifetime_value)\"] = np.log(df_bivariate_analysis[\"lifetime_value\"])\n",
    "df_bivariate_analysis[\"log(income)\"] = np.log(df_bivariate_analysis[\"income\"])\n",
    "\n",
    "#drop old columns that were transformed into the new ones\n",
    "df_bivariate_analysis = df_bivariate_analysis.drop(df_bivariate_analysis[[\"income\",\n",
    "                                                                      \"lifetime_value\",\n",
    "                                                                      \"total_dollar_redeemed\",\n",
    "                                                                      \"total_points_redeemed\"]] \n",
    "                                                                      , axis=1)\n",
    "\n",
    "\n",
    "numeric_cols = [\n",
    "    \"total_flights\", \"median_flights_per_month\", \"std_flights_per_month\",\n",
    "    \"total_flights_w_comp\", \"ratio_comp_per_flight\", \"median_flights_w_comp\", \"std_flights_w_comp\",\n",
    "    \"total_distance_km\", \"avg_distance_per_flight\", \"median_distance_km\", \"std_distance_km\",\n",
    "    \"total_points_acc\", \"avg_points_acc_per_flight\", \"median_points_acc\", \"std_points_acc\",\n",
    "    \"log(total_points_redeemed)\", \"log(total_dollar_redeemed)\",\n",
    "    \"log(lifetime_value)\", \"log(income)\", \"active_months\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f80e986-92dc-41a3-bb47-1112456d6d20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Pearson Correlation (post-transform)\n",
    "Linear correlation across numeric variables after accounting for skew through the available log-transformed features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c69f4f7b-992c-4590-81cf-bd0be4bf6b9d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "pearson correlation heatmap"
    }
   },
   "outputs": [],
   "source": [
    "num_for_pearson = []\n",
    "for col in numeric_cols:\n",
    "    # try to find a log version formatted either style\n",
    "    log_paren = f\"log({col})\"\n",
    "    log_under = f\"log_{col}\"\n",
    "    \n",
    "    if log_paren in df_bivariate_analysis.columns:\n",
    "        num_for_pearson.append(log_paren)\n",
    "    elif log_under in df_bivariate_analysis.columns:\n",
    "        num_for_pearson.append(log_under)\n",
    "    else:\n",
    "        num_for_pearson.append(col)\n",
    "\n",
    "# ensure uniqueness while preserving order\n",
    "num_for_pearson = list(dict.fromkeys(num_for_pearson))\n",
    "\n",
    "# compute correlation\n",
    "corr_pear = df_bivariate_analysis[num_for_pearson].replace([np.inf, -np.inf], np.nan).dropna().corr()\n",
    "\n",
    "plt.figure(figsize=(0.55 * len(num_for_pearson) + 5, 0.55 * len(num_for_pearson) + 5))\n",
    "sns.heatmap(\n",
    "    corr_pear,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"vlag\",\n",
    "    center=0,\n",
    "    linewidths=.5,\n",
    "    alpha=0.65,\n",
    "    annot_kws={\"size\": 10},\n",
    ")\n",
    "plt.title(\"Pearson correlation — numeric features (log where available)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcda5496-ed3f-4d15-b569-0766179ad47d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Spearman vs. Pearson — Summary\n",
    "\n",
    "There are **no major differences** between the Spearman and Pearson correlation results.\n",
    "\n",
    "- The **strong** relationships between travel volume, distance, tenure, and points remain **consistent**\n",
    "- The **weak** relationships for `log(lifetime_value)` and `log(income)` also stay **weak**\n",
    "- Log-transformations helped normalize distributions, but did **not** fundamentally change linear vs. monotonic patterns\n",
    "\n",
    " **Conclusion**:  \n",
    "> The core relationships in our dataset are both **linear** and **monotonic**, so results from Spearman and Pearson are aligned.  \n",
    "> No additional interpretation changes are necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca959f14-fa56-4183-96e3-ea379528d2a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Boxplots: Categorical → Numeric (2×2)\n",
    "Segment-wise distribution comparisons for key metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4240317-ae4d-4d28-a012-96407a357155",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "boxplots"
    }
   },
   "outputs": [],
   "source": [
    "# Fixed categorical feature list (based on your heatmap)\n",
    "cat_cols = [\n",
    "    \"gender\",\n",
    "    \"marital_status\",\n",
    "    \"education\",\n",
    "    \"loyalty_status\",\n",
    "    \"enrollment_type\",\n",
    "    \"is_enrolled\"\n",
    "]\n",
    "\n",
    "# Relevant numeric variables you already defined/used\n",
    "relevant_num_cols = [\n",
    "    \"total_flights\", \"income\", \"active_months\", \"lifetime_value\", \"total_points_redeemed\"\n",
    "]\n",
    "relevant_num_cols = [c for c in relevant_num_cols if c in df_cf_unique.columns]\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_palette(\"vlag\")  \n",
    "\n",
    "# One figure (grid) per categorical variable\n",
    "for cat in cat_cols:\n",
    "    n = len(relevant_num_cols)\n",
    "    rows = math.ceil(n / 3)\n",
    "    cols = min(3, n)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(16, 5 * rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax, num in zip(axes, relevant_num_cols):\n",
    "        sns.boxplot(\n",
    "            data=df_cf_unique,\n",
    "            x=cat,\n",
    "            y=num,\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.set_title(f\"{num} by {cat}\")\n",
    "        ax.tick_params(axis=\"x\", rotation=25)\n",
    "\n",
    "    for ax in axes[n:]:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "    fig.suptitle(f\"Numeric feature distributions by {cat}\", fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcc8fe13-5c34-4d59-b574-200d237a3a88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Boxplot Interpretation — Summary\n",
    "\n",
    "- **Active customers fly more** → expected and clearly visible  \n",
    "- **Standard enrollment leads to more flights** than promotion-based sign-ups  \n",
    "- **Loyalty status relates to lifetime value** → higher tier = higher value  \n",
    "- **Education clearly relates to income**, marital status slightly so  \n",
    "\n",
    "However:\n",
    "- **Income and lifetime value do not correlate** with the main travel metrics  \n",
    "  (as seen in the Pearson/Spearman correlations earlier)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdc62ab7-846d-4b4e-8ea7-368ccabafaf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Stacked Bars (Composition, %)\n",
    "Compositional view of key category interactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14d9add3-1bc8-466e-ac69-f9074328a405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Why stacked percent bar charts?\n",
    "\n",
    "They show **how one categorical variable is distributed within another**  \n",
    "→ useful to spot **group differences**, **patterns**, and **imbalances** in the data.\n",
    "\n",
    "(Visual complement to Cramér’s V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8019a5c-613d-45cd-b38a-60edabe62f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Key categorical variables to include\n",
    "cat_cols = [\n",
    "    \"gender\",\n",
    "    \"marital_status\",\n",
    "    \"education\",\n",
    "    \"loyalty_status\",\n",
    "    \"enrollment_type\",\n",
    "    \"is_enrolled\"\n",
    "]\n",
    "\n",
    "# Percent helper\n",
    "def stacked_percent(df, group_col, stack_col):\n",
    "    tmp = df[[group_col, stack_col]].dropna()\n",
    "    if tmp.empty:\n",
    "        return None\n",
    "    return (\n",
    "        tmp.groupby([group_col, stack_col]).size()\n",
    "        .groupby(level=0).apply(lambda x: x / x.sum() * 100)\n",
    "        .unstack(fill_value=0)\n",
    "    )\n",
    "\n",
    "# limited key pairings (more relevant for readability)\n",
    "pairings = [\n",
    "    (\"education\", \"loyalty_status\"),\n",
    "    (\"gender\", \"enrollment_type\"),\n",
    "    (\"education\", \"gender\"),\n",
    "    (\"is_enrolled\", \"loyalty_status\"),\n",
    "    (\"marital_status\", \"education\"),\n",
    "    (\"enrollment_type\", \"is_enrolled\"),\n",
    "    (\"gender\", \"loyalty_status\"),\n",
    "    (\"marital_status\", \"enrollment_type\")\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(15, 18))\n",
    "axes = axes.ravel()\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"vlag\")  \n",
    "\n",
    "for ax, (g, s) in zip(axes, pairings):\n",
    "    dist = stacked_percent(df_bivariate_analysis, g, s)\n",
    "    if dist is not None:\n",
    "        dist.plot(kind=\"bar\", stacked=True, ax=ax, alpha=0.85)\n",
    "        ax.set_title(f\"{s} distribution by {g}\")\n",
    "        ax.set_xlabel(g)\n",
    "        ax.set_ylabel(\"Percent\")\n",
    "        ax.tick_params(axis=\"x\", rotation=25)\n",
    "        ax.legend(title=s, bbox_to_anchor=(1.02, 1))\n",
    "    else:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e886e2f8-cc0b-463b-b7b9-06753535d1a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Cramér’s V (Categorical–Categorical Association)\n",
    "Association strength between categorical features using bias-corrected Cramér’s V.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6d3fd8e-6e5d-40ef-9a18-f36776862fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Cramér’s V — Measuring Association Between Categorical Variables\n",
    "\n",
    "**Cramér’s V** is a statistical measure that quantifies how strongly two **categorical variables** are associated.\n",
    "\n",
    "### Concept Overview\n",
    "- It is based on the **Chi-square (χ²) statistic**, which compares the **observed frequencies** in a contingency table to the **expected frequencies** (if the variables were independent).  \n",
    "- Cramér’s V normalizes this χ² value to produce a result **between 0 and 1**, making it easy to interpret.\n",
    "\n",
    "\\[\n",
    "V = \\sqrt{\\frac{\\chi^2}{n \\times (k - 1)}}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- **χ²** = Chi-square statistic from the contingency table  \n",
    "- **n** = total number of observations  \n",
    "- **k** = smaller of (number of rows, number of columns)\n",
    "\n",
    "### Interpretation\n",
    "| Value of V | Interpretation |\n",
    "|-------------|----------------|\n",
    "| 0.00 | No association |\n",
    "| 0.10–0.30 | Weak association |\n",
    "| 0.30–0.50 | Moderate association |\n",
    "| > 0.50 | Strong association |\n",
    "\n",
    "### Key Points\n",
    "- Symmetric: same result whether you swap the variables.  \n",
    "- Works only for **categorical–categorical** relationships.  \n",
    "- Does **not** tell you the *direction* of association (only strength).  \n",
    "- Useful for **feature selection** or **EDA** when dealing with non-numeric data.\n",
    "\n",
    "\n",
    "\n",
    "> **Example:**  \n",
    "> Measuring how strongly `loyalty_status` is associated with `marital_status`  \n",
    "> → High V = membership levels differ systematically by marital status  \n",
    "> → Low V = no mean:ingful difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "824818ad-473c-4db4-b2e9-930d6e1785fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cramér's V for categorical features\n",
    "cats = [c for c in categorical_features if c in df_bivariate_analysis.columns]\n",
    "\n",
    "def cramers_v(x, y):\n",
    "    tbl = pd.crosstab(x, y)\n",
    "    if tbl.shape[0] < 2 or tbl.shape[1] < 2:\n",
    "        return np.nan\n",
    "    chi2 = chi2_contingency(tbl, correction=False)[0]\n",
    "    n = tbl.values.sum()\n",
    "    r, k = tbl.shape\n",
    "    phi2 = chi2 / n\n",
    "    # bias correction\n",
    "    phi2_corr = max(0, phi2 - (k - 1)*(r - 1)/(n - 1))\n",
    "    r_corr = r - (r - 1)**2/(n - 1)\n",
    "    k_corr = k - (k - 1)**2/(n - 1)\n",
    "    denom = max((k_corr - 1), (r_corr - 1))\n",
    "    return np.sqrt(phi2_corr / denom) if denom > 0 else np.nan\n",
    "\n",
    "if len(cats) > 1:\n",
    "    m = pd.DataFrame(index=cats, columns=cats, dtype=float)\n",
    "\n",
    "    for a in cats:\n",
    "        for b in cats:\n",
    "            if a == b:\n",
    "                m.loc[a, b] = 1.0\n",
    "            else:\n",
    "                m.loc[a, b] = cramers_v(df_bivariate_analysis[a], df_bivariate_analysis[b])\n",
    "\n",
    "    plt.figure(figsize=(min(0.7 * len(cats) + 5, 14),\n",
    "                        min(0.7 * len(cats) + 5, 14)))\n",
    "\n",
    "    sns.heatmap(\n",
    "        m.astype(float),\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cmap=\"vlag\",\n",
    "        vmin=0, vmax=1,\n",
    "        linewidths=.5,\n",
    "        annot_kws={\"size\": 10},\n",
    "        alpha=0.65,\n",
    "        cbar_kws={\"shrink\": 0.85}\n",
    "    )\n",
    "\n",
    "    plt.title(\"Cramér's V — categorical features\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough categorical features for Cramér's V.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f8e6ce0-1a25-4503-9d40-acd3bc308d48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Cramér’s V — Interpretation\n",
    "\n",
    "- No strong associations among categorical variables.  \n",
    "- Only a **weak link** between `marital_status` and `education` (V ≈ 0.21).  \n",
    "- All others (`gender`, `loyalty_status`, `enrollment_type`, `is_enrolled`) show **near-zero** association.  \n",
    "\n",
    "> Categorical features are largely independent — no redundancy or multicollinearity concerns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3112f70c-bf0c-4a85-8a42-65397679b15a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install jinja2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "137649e0-f6f0-4e9d-a3e0-d698587bde54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Correlation Ratio (η) for Selected Pairs\n",
    "We report η for selected categorical→numeric pairs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b0e3091-e2ef-444e-9fde-660c555ed73b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Correlation Ratio (η) — Categorical → Numeric Association\n",
    "\n",
    "The **Correlation Ratio (η)** measures how strongly a **categorical variable** explains the variance of a **numeric variable**.\n",
    "\n",
    "- It captures **non-linear** relationships better than Pearson correlation.  \n",
    "- η ranges from **0 to 1**:\n",
    "  - **0** → No relationship (the numeric mean is similar across all categories)  \n",
    "  - **1** → Perfect relationship (each category has a distinct numeric value)\n",
    "\n",
    "\\[\n",
    "\\eta = \\sqrt{\\frac{\\text{between-group variance}}{\\text{total variance}}}\n",
    "\\]\n",
    "\n",
    "\n",
    "### Interpretation\n",
    "- **High η** → The numeric variable changes meaningfully across categories  \n",
    "  (e.g., `loyalty_status` strongly affects `totalpoints`)  \n",
    "- **Low η** → The numeric variable is distributed similarly across all categories  \n",
    "  (no practical relationship)\n",
    "\n",
    "Used for:  \n",
    "Assessing how much variation in a **continuous feature** is explained by a **categorical grouping** — e.g., how `education` affects `income`, or how `loyalty_status` influences `lifetime_value`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed141a42-0631-4172-9e54-4490908c04ab",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Correlation ratio (η)"
    }
   },
   "outputs": [],
   "source": [
    "# Correlation ratio (η) using log-transformed numeric features\n",
    "def sanitize_numeric(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "def correlation_ratio(categories, values):\n",
    "    cat = pd.Series(categories)\n",
    "    num = sanitize_numeric(values)\n",
    "    \n",
    "    mask = cat.notna() & num.notna()\n",
    "    cat = cat[mask]\n",
    "    num = num[mask]\n",
    "\n",
    "    if cat.nunique() < 2:\n",
    "        return np.nan\n",
    "\n",
    "    means = num.groupby(cat).mean()\n",
    "    counts = num.groupby(cat).size()\n",
    "    mean_total = num.mean()\n",
    "\n",
    "    ss_between = ((means - mean_total) ** 2 * counts).sum()\n",
    "    ss_total = ((num - mean_total) ** 2).sum()\n",
    "\n",
    "    return np.sqrt(ss_between / ss_total) if ss_total > 0 else np.nan\n",
    "\n",
    "\n",
    "# mapping → log columns\n",
    "pairs = [\n",
    "    (\"marital_status\", \"log(income)\"),\n",
    "    (\"education\", \"log(income)\"),\n",
    "    (\"loyalty_status\", \"avg_distance_per_flight\"),  # already a ratio, no log needed\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for cat_col, num_col in pairs:\n",
    "    if cat_col in df_bivariate_analysis.columns and num_col in df_bivariate_analysis.columns:\n",
    "        eta = correlation_ratio(df_bivariate_analysis[cat_col],\n",
    "                                df_bivariate_analysis[num_col])\n",
    "        results.append((f\"{cat_col} → {num_col}\", eta))\n",
    "    else:\n",
    "        results.append((f\"{cat_col} → {num_col}\", np.nan))\n",
    "\n",
    "\n",
    "eta_df = pd.DataFrame(results, columns=[\"Pair\", \"Eta\"])\n",
    "display(eta_df.style.format({\"Eta\": \"{:.3f}\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68d5a143-8011-45ed-936c-92fc9e8abeb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Correlation Ratio (η) — Interpretation Linked to Boxplots\n",
    "\n",
    "- **education → log(income)** has a **strong relationship (η = 0.66)**  \n",
    "  → Clearly visible in the boxplots: income high for bachelor education, low for college students and leveled for the other categories\n",
    "\n",
    "- **marital_status → log(income)** shows a **very weak effect (η = 0.06)**  \n",
    "  → Boxplots show slight differences (married/divorced slightly higher),  \n",
    "\n",
    "- **loyalty_status → avgdistance** is **negligible (η = 0.02)**  \n",
    "  → Matches the boxplots: loyalty tiers do **not** differ much in travel distance\n",
    "\n",
    " **Conclusion:**  \n",
    "The correlation ratio confirms what we see visually —  \n",
    "only the **education → income** link is truly meaningful,  \n",
    "while the others show **little to no practical association**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efa1fa7a-31ab-4c2b-9476-6253d73b85b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Geospatial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b5754c42-3ff0-421c-b2a1-264b0ed4cb1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cf_unique.info()\n",
    "geo_data = df_cf_unique[['prov_or_state', 'city', 'latitude', 'longitude', 'postal_code', 'location_code']]\n",
    "geo_data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "28095489-41aa-40ee-9299-042305442f92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Check for Inconsistencies in Geographical Data: Start from a few unique values to lots of unique values to ensure that everything is covered\n",
    "# Find prov_or_state that appear in multiple location_code\n",
    "location_code_prov = df_cf_unique.groupby('prov_or_state')['location_code'].nunique() # sums the amount of unique location_code values per prov_or_state\n",
    "location_code_in_multiple_provs = location_code_prov[location_code_prov > 1].index.tolist() # filter prov_or_state that appear in multiple location_code\n",
    "print(f\"{len(location_code_in_multiple_provs)} provinces/states appear in multiple location codes.\")\n",
    "\n",
    "# Find cities that appear in multiple location_code\n",
    "location_code_city = df_cf_unique.groupby('city')['location_code'].nunique() # sums the amount of unique location_code values per city\n",
    "location_code_in_multiple_cities = location_code_city[location_code_city > 1].index.tolist() # filter cities that appear in multiple location_code\n",
    "print(f\"{len(location_code_in_multiple_cities)} cities appear in multiple location codes.\")\n",
    "\n",
    "# Find postal_code that appear in multiple location_code\n",
    "location_code_postal = df_cf_unique.groupby('postal_code')['location_code'].nunique() # sums the amount of unique location_code values per postal_code\n",
    "location_code_in_multiple_postals = location_code_postal[location_code_postal > 1].index.tolist() # filter postal_code that appear in multiple location_code\n",
    "print(f\"{len(location_code_in_multiple_postals)} postal codes appear in multiple location codes.\")\n",
    "print(\"==> Even on the postal codes level, each of the 3 location_codes is present in all entries (55 out of 55). This means that every postal code and therefore every city has rural, urban and suburban areas\")\n",
    "# TODO check whether this is really true and if so, the location code feature has to be dropped or transformed manually\n",
    "\n",
    "# Find cities that appear in more than one province/state\n",
    "city_prov = df_cf_unique.groupby('city')['prov_or_state'].nunique() # sums the amount of unique prov_or_state values per city\n",
    "city_in_multiple_provs = city_prov[city_prov > 1].index.tolist() # filter cities that appear in more than one province/state\n",
    "print(f\"{len(city_in_multiple_provs)} cities appear in multiple provinces/states.\")\n",
    "\n",
    "# Find postal codes that appear in more than one city\n",
    "postal_code_city = df_cf_unique.groupby('postal_code')['city'].nunique() # sums the amount of unique city values per postal code\n",
    "postal_code_in_multiple_cities = postal_code_city[postal_code_city > 1].index.tolist() # filter postal codes that appear in more than one city\n",
    "print(f\"{len(postal_code_in_multiple_cities)} postal codes appear in multiple cities.\")\n",
    "\n",
    "\n",
    "# Find pairs of longitude and latitude that are assigned to more than one city\n",
    "print(f\"Unique latitude values: {df_cf_unique['latitude'].nunique()} || Unique longitude values: {df_cf_unique['longitude'].nunique()}\")\n",
    "unique_lat_lon_pairs = df_cf_unique[['latitude', 'longitude']].drop_duplicates() # determine the pairs by dropping duplicates\n",
    "print(f\"Number of unique (latitude, longitude) pairs: {len(unique_lat_lon_pairs)}\")\n",
    "\n",
    "lat_lon_city_counts = df_cf_unique.groupby(['latitude', 'longitude'])['city'].nunique() # sums the amount of unique city values per (latitude, longitude) pair\n",
    "lat_lon_multiple_cities = lat_lon_city_counts[lat_lon_city_counts > 1] # filter pairs that appear in more than one city\n",
    "print(f\"{len(lat_lon_multiple_cities)} (latitude, longitude) pairs are assigned to more than one city.\")\n",
    "if len(lat_lon_multiple_cities) > 0:\n",
    "    print(lat_lon_multiple_cities)\n",
    "df_cf_unique.groupby(['latitude', 'longitude'])['city'].unique() # show the 1:1 match of coordinates to their cities\n",
    "\n",
    "\n",
    "# Findings:\n",
    "# - even on the postal codes level, each of the 3 location_codes is present in all entries (55 out of 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "12f44385-35e9-4e11-8246-be6c67354641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Create a Geo Plot (Regional patterns in behavior)\n",
    "# Create a geo plot using the pairs of latitude and longitude to see where customers fly the most according to their total distance travelled and total flights\n",
    "geo_agg = df_cf_unique.groupby(['latitude', 'longitude', 'city'], as_index=False).agg({\n",
    "    'total_distance_km': 'median',\n",
    "    'total_flights': 'median'\n",
    "}).rename(columns={'total_distance_km': 'totaldistance_median', 'total_flights': 'totalflights_median'}) # Aggregate total distance and total flights per (latitude, longitude) pair\n",
    "display(geo_agg)\n",
    "\n",
    "# Plot using scatter plot (size = total_distance_km, color = total_flights)\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(\n",
    "    geo_agg['longitude'], geo_agg['latitude'],\n",
    "    s=geo_agg['totaldistance_median'] / 500,  # You can't see a difference in the size but its telling the truth about the data because the values for total_distance_km are close to each other (Tufte requirement)\n",
    "    c=geo_agg['totalflights_median'],\n",
    "    cmap='viridis', alpha=0.7, edgecolor='k'\n",
    ")\n",
    "plt.colorbar(scatter, label='Total Flights')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Customer Flight Activity by Location\\n(Size: Total Distance, Color: Total Flights)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(geo_agg['totaldistance_median'] / 500)\n",
    "\n",
    "# Findings:\n",
    "# - In Sudbury there are by far the least median total flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "687fff7e-2a57-4d9b-8c22-a48b9d78630e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Geo-Plot customer counts per (latitude, longitude) as dot size\n",
    "counts = df_cf_unique.groupby(['latitude', 'longitude'], as_index=False).size().rename(columns={'size':'customer_count'}).reset_index(drop=True)\n",
    "print(sum(counts['customer_count']) == len(df_cf_unique)) # check if counts sum up to total number of customers\n",
    "geo_agg = geo_agg.merge(counts, on=['latitude', 'longitude'], how='left') # merge counts into geo_agg to have all relevant data in one dataframe\n",
    "# Merge median income and customer lifetime value per location into geo_agg\n",
    "income_ltv = df_cf_unique.groupby(['latitude', 'longitude'], as_index=False).agg({\n",
    "    'income': 'median',\n",
    "    'lifetime_value': 'median'\n",
    "}).rename(columns={'income': 'income_median', 'lifetime_value': 'lifetime_value_median'})\n",
    "geo_agg = geo_agg.merge(income_ltv, on=['latitude', 'longitude'], how='left')\n",
    "display(geo_agg.sort_values('customer_count', ascending=False)) # Display top locations by customer count\n",
    "\n",
    "# Compare scaled sd to display the feature with a bigger difference betweeen the customers: Min-max scale income_median and lifetime_value_median to compare spreads on the same scale\n",
    "geo_agg['income_median_scaled'] = (geo_agg['income_median'] - geo_agg['income_median'].min()) / (geo_agg['income_median'].max() - geo_agg['income_median'].min())\n",
    "geo_agg['lifetime_value_median_scaled'] = (geo_agg['lifetime_value_median'] - geo_agg['lifetime_value_median'].min()) / (geo_agg['lifetime_value_median'].max() - geo_agg['lifetime_value_median'].min())\n",
    "print(f\"Std (raw) income_median: {geo_agg['income_median'].std():.2f}, median_ltv: {geo_agg['lifetime_value_median'].std():.2f}\")\n",
    "print(f\"Std (scaled 0-1) income_median: {geo_agg['income_median_scaled'].std():.4f}, median_ltv: {geo_agg['lifetime_value_median_scaled'].std():.4f}\")\n",
    "# => income_median has a higher spread than lifetime_value_median so we will use income_median for the color coding in the plot\n",
    "\n",
    "# Scale sizes for visualization \n",
    "location_dot_sizes = (geo_agg['customer_count'])/2 # (np.sqrt(geo_agg['customer_count'])) * 40 # sqrt helps reduce skew from large counts\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sc = plt.scatter(\n",
    "    geo_agg['longitude'], geo_agg['latitude'],\n",
    "    s=location_dot_sizes,\n",
    "    c=geo_agg['income_median'],\n",
    "    cmap='Blues',\n",
    "    vmin=0,\n",
    "    vmax=geo_agg['income_median'].max(),\n",
    "    alpha=0.7,\n",
    "    edgecolor='k',\n",
    "    linewidth=0.4\n",
    ")\n",
    "plt.colorbar(label='Median Income')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Number of Customers by Location (dot size ~ customer_count; color ~ income_median)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Findings:\n",
    "# - by far the most customers in Toronto, Vancouver and Montreal (which are also the 3 most-populated cities in Canada but Montreal has more inhabitants than Vancouver)\n",
    "# - income_median and lifetime_value_median both do not vary much between locations # TODO find a feature that varies more for more impactful visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "cef8836b-4ccd-47ac-ab22-a2c8eb817570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Choropleth of Canada by province (customer counts)\n",
    "import plotly.express as px\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Aggregate by province\n",
    "prov_agg = df_cf_unique.groupby('prov_or_state', as_index=False).agg(\n",
    "    customer_count=('loyalty_id', 'nunique'),\n",
    "    income_median=('income', 'median'),\n",
    "    lifetime_value_median=('lifetime_value', 'median')\n",
    ")\n",
    "\n",
    "# Map province names to abbreviations\n",
    "province_abbr = {\n",
    "    \"Alberta\": \"AB\",\n",
    "    \"British Columbia\": \"BC\",\n",
    "    \"Manitoba\": \"MB\",\n",
    "    \"New Brunswick\": \"NB\",\n",
    "    \"Newfoundland\": \"NL\",\n",
    "    \"Nova Scotia\": \"NS\",\n",
    "    \"Ontario\": \"ON\",\n",
    "    \"Prince Edward Island\": \"PE\",\n",
    "    \"Quebec\": \"QC\",\n",
    "    \"Saskatchewan\": \"SK\",\n",
    "    \"Yukon\": \"YT\"\n",
    "}\n",
    "prov_agg[\"province_abbr\"] = prov_agg[\"prov_or_state\"].map(province_abbr)\n",
    "prov_agg = prov_agg.sort_values(by=\"customer_count\", ascending=False)\n",
    "display(prov_agg)\n",
    "# Downloaded map from https://gist.github.com/M1r1k/d5731bf39e1dfda5b53b4e4c560d968d/\n",
    "canada_geo = json.load(open(\"data/canada_provinces.geo.json\"))\n",
    "\n",
    "# Create choropleth (featureidkey matches \"properties.name\" in this geojson)\n",
    "fig = px.choropleth(\n",
    "    prov_agg,\n",
    "    geojson=canada_geo,\n",
    "    locations=\"prov_or_state\",\n",
    "    featureidkey=\"properties.name\",\n",
    "    color=\"customer_count\",\n",
    "    color_continuous_scale=\"Blues\",\n",
    "    hover_data=[\"customer_count\", \"income_median\", \"lifetime_value_median\", \"province_abbr\"],\n",
    "    labels={\"customer_count\": \"Unique customers\", \"province_abbr\": \"Abbreviation\"},\n",
    ")\n",
    "\n",
    "fig.update_geos(fitbounds=\"locations\", visible=False)\n",
    "fig.update_layout(\n",
    "    title_text=\"Customer Counts by Province (Canada)\",\n",
    "    margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0},\n",
    "    coloraxis_colorbar={\"title\":\"Customers\"}\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Findings:\n",
    "# - By far the most customers are in Ontario, British Columbia and Quebec\n",
    "# - 2 provinces are missing in the dataset: Northwest Territories (NT) and Nunavut (NU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "176c09da-6721-4439-bf69-18edd48b3d05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Alternative GeoJSON\n",
    "with open(\"data/georef-canada-province.geojson\", \"r\") as f:\n",
    "    canada_province_geo = json.load(f)\n",
    "\n",
    "# Inspect the first feature to see available properties of the geojson\n",
    "print(\"Available properties in GeoJSON:\")\n",
    "print(canada_province_geo['features'][0]['properties'].keys())\n",
    "prov_names = [feat[\"properties\"].get(\"prov_name_en\") for feat in canada_province_geo[\"features\"]]\n",
    "print(sorted({name for name in prov_names if name}))\n",
    "print(sorted(prov_agg[\"prov_or_state\"].unique()))\n",
    "\n",
    "# Create the choropleth an adjust featureidkey based on what is printed above\n",
    "fig2 = px.choropleth(\n",
    "    prov_agg,\n",
    "    geojson=canada_province_geo,\n",
    "    locations=\"prov_or_state\",\n",
    "    featureidkey=\"properties.prov_name_en\",\n",
    "    color=\"customer_count\",\n",
    "    color_continuous_scale=\"Greens\",\n",
    "    hover_data=[\"customer_count\", \"income_median\", \"lifetime_value_median\", \"prov_or_state\"],\n",
    "    labels={\"customer_count\": \"Unique customers\", \"prov_or_state\": \"Province\"},\n",
    ")\n",
    "\n",
    "fig2.update_geos(fitbounds=\"locations\", visible=True)\n",
    "fig2.update_traces(marker_line_color=\"black\", marker_line_width=1)\n",
    "\n",
    "fig2.update_layout(\n",
    "    title_text=\"Customer Counts by Province (Canada, georef-canada-province.geojson)\",\n",
    "    margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0},\n",
    "    coloraxis_colorbar={\"title\":\"Customers\"},\n",
    "    height=600,\n",
    "    width=1400\n",
    ")\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa721fdb-32ea-4a2e-9b4a-e965bd707848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c1e5c8c-ecbb-4f88-9042-ad2afec869bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Why Time Series Analysis?\n",
    "\n",
    "Time series help us understand how **customer behavior changes over time**.  \n",
    "By tracking metrics like flights, distance, or points per month, we can:\n",
    "\n",
    "- Spot **seasonal travel patterns** (holidays, summer)\n",
    "- Detect **behavior shifts** due to external factors (e.g., COVID-19)\n",
    "- Monitor **loyalty engagement trends** (are customers flying more over time?)\n",
    "- Compare how different **customer segments evolve** (e.g., income groups, education)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "879c4140-5b7c-4aa7-85ff-ce7d3cffbcac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_customerflights)\n",
    "#print(df_customerflights.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86e172c0-af38-4ec2-a546-893f5b679f2f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "histogram: when do customers join the loyalty program?"
    }
   },
   "outputs": [],
   "source": [
    "# Enrollment Trend Analysis: When Do Customers Join the Loyalty Program?\n",
    "import plotly.express as px\n",
    "\n",
    "# Work on a copy to avoid modifying original data\n",
    "df_enroll = df_customerflights.copy()\n",
    "\n",
    "# Ensure enrollment_date is properly recognized as datetime\n",
    "df_enroll['enrollment_date'] = pd.to_datetime(df_enroll['enrollment_date'])\n",
    "\n",
    "# Convert daily dates to monthly period for a cleaner time-based trend analysis\n",
    "df_enroll['enrollment_month'] = (\n",
    "    df_enroll['enrollment_date'].dt.to_period('M').dt.to_timestamp()\n",
    ")\n",
    "\n",
    "# Count number of new unique enrollments per month\n",
    "df_month = (\n",
    "    df_enroll.groupby('enrollment_month', as_index=False)\n",
    "    .agg(num_customers=('loyalty_id', 'nunique'))\n",
    ")\n",
    "\n",
    "# Visualize enrollment volume per month\n",
    "fig = px.bar(\n",
    "    df_month,\n",
    "    x=\"enrollment_month\",\n",
    "    y=\"num_customers\",\n",
    "    title=\"New Loyalty Program Enrollments per Month\",\n",
    "    labels={\"enrollment_month\": \"Month\", \"num_customers\": \"New Customers\"},\n",
    ")\n",
    "\n",
    "# Improve readability & interactivity\n",
    "fig.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    title_x=0.5,\n",
    "    hovermode=\"x unified\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b483dc47-9a66-49cb-b0bb-86eed2a81c08",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "TSA: core flight and points metrics"
    }
   },
   "outputs": [],
   "source": [
    "# Monthly Trends: Core Flight and Points Metrics\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Work on a safe copy\n",
    "df_ts = df_customerflights.copy()\n",
    "\n",
    "# Metrics to evaluate over time\n",
    "metrics = [\n",
    "    'num_flights',\n",
    "    'num_flights_with_companions',\n",
    "    'distance_km',\n",
    "    'points_accumulated',\n",
    "    'points_redeemed',\n",
    "    'dollar_cost_points_redeemed'\n",
    "]\n",
    "\n",
    "# Aggregate customer metrics per month (mean values)\n",
    "df_monthly = (\n",
    "    df_ts.groupby('year_month_date', as_index=False)[metrics]\n",
    "         .mean()\n",
    ")\n",
    "\n",
    "# Create subplot grid: one row per metric\n",
    "fig = make_subplots(\n",
    "    rows=len(metrics),\n",
    "    cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.05,\n",
    "    subplot_titles=[f\"Average {m} per Month\" for m in metrics]\n",
    ")\n",
    "\n",
    "# Plot each metric as its own time series\n",
    "for i, metric in enumerate(metrics, start=1):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_monthly['year_month_date'],\n",
    "            y=df_monthly[metric],\n",
    "            mode=\"lines+markers\",\n",
    "            name=metric,\n",
    "        ),\n",
    "        row=i, col=1\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=280 * len(metrics),\n",
    "    title=\"Customer Activity Over Time (Monthly Averages)\",\n",
    "    title_x=0.5,\n",
    "    template=\"plotly_white\",\n",
    "    hovermode=\"x unified\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Date\")\n",
    "fig.update_yaxes(title_text=\"Average Value\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57304cb4-0223-41e9-9201-9bf8d9ebb175",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "TSA: customer demographics comparison"
    }
   },
   "outputs": [],
   "source": [
    "# Segmented Time Series: Distance by Customer Demographics\n",
    "import plotly.express as px\n",
    "\n",
    "df_seg = df_customerflights.copy()\n",
    "\n",
    "# Income quantiles to create comparable segments\n",
    "df_seg['income_quantile'] = pd.qcut(\n",
    "    df_seg['income'], \n",
    "    q=4, \n",
    "    labels=[#'Q1 (low)', \n",
    "            'Q2', \n",
    "            'Q3', \n",
    "            'Q4 (high)'],\n",
    "    duplicates='drop'\n",
    ")\n",
    "\n",
    "# Categorical segments to compare\n",
    "segments = ['gender', 'education', 'location_code', 'income_quantile', 'marital_status']\n",
    "\n",
    "for feature in segments:\n",
    "\n",
    "    # Compute monthly avg distance by segment group\n",
    "    df_grouped = (\n",
    "        df_seg.groupby(['year_month_date', feature], as_index=False)\n",
    "              .agg(avg_distance_km=('distance_km', 'mean'))\n",
    "    )\n",
    "\n",
    "    # Rolling smoothing to highlight trends (3 months)\n",
    "    df_grouped['avg_distance_smooth'] = (\n",
    "        df_grouped.groupby(feature)['avg_distance_km']\n",
    "                  .transform(lambda x: x.rolling(3, center=True).mean())\n",
    "    )\n",
    "\n",
    "    fig = px.line(\n",
    "        df_grouped,\n",
    "        x=\"year_month_date\",\n",
    "        y=\"avg_distance_smooth\",\n",
    "        color=feature,\n",
    "        title=f\"Monthly Average Distance by {feature.capitalize()}\",\n",
    "        labels={\"avg_distance_smooth\": \"Avg Distance (3-month mean)\"}\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        title_x=0.5,\n",
    "        hovermode=\"x unified\",\n",
    "        height=420,\n",
    "        legend_title_text=feature.capitalize()\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "467ca12b-9865-46bd-b598-16110f9b8d7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Time Series Insights — Summary\n",
    "\n",
    "Across all flight-related metrics, we observe:\n",
    "\n",
    "**Strong seasonal patterns**\n",
    "- Clear peaks in **summer**, around **Easter**, and in **December** (holiday travel)\n",
    "\n",
    "**Overall growing trend**\n",
    "- Flight activity gradually increases over the observed years\n",
    "\n",
    "**COVID expectation**\n",
    "- A dip in early 2020 would be expected due to the pandemic\n",
    "- This effect is **not clearly visible** in our data\n",
    "\n",
    "**Customer groups behave similarly**\n",
    "- Gender, education, income, etc. show **almost identical** temporal travel patterns\n",
    "- → No segment-specific behavioral shifts over time\n",
    "\n",
    "**Promotion-driven surge**\n",
    "- A noticeable increase in **new loyalty enrollments in 2021**\n",
    "- Likely linked to **a targeted promotion campaign** that successfully attracted new members\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**  \n",
    "Customer travel behavior is driven primarily by **seasonality and overall growth**.  \n",
    "The **2021 enrollment spike** reflects **campaign success**, rather than structural behavioral changes in customer segments.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "main_notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "DM_Fall2526",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
